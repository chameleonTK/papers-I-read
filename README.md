
# Papers I read

Summaries and notes on recent cross-lingual approaches and South-east Asian languages

## Cross-lingual approaches
* ðŸ““ [Understanding Back-Translation at Scale](./cross-lingual/2018-Edunov_back-translation-at-scale.md) \#\# `back-translation`, `data-augmentation`, `unsupervised`
* ðŸ““ [Cross-lingual Projections between Languages from Different Families](./cross-lingual/2013-Mo_Xprojection.md) \#\# `data-augmentation`

* ðŸ““ [Cross-lingual Language Model Pretraining](./cross-lingual/2019-Lample_XLM.md) \#\# `pre-trained`, `multilingual`, `XLM`

* ðŸ““ [Unsupervised Machine Translation Using Monolingual Corpora Only](./cross-lingual/2017-Lample_UNMT-only-monolingual-corpus.md) \#\# `machine-translation`, `NMT`, `unsupervised`

* ðŸ““ [Unsupervised Neural Machine Translation](./cross-lingual/2017-Artetxe_UNMT.md) \#\# `machine-translation`, `NMT`, `unsupervised`




## Relavent Datasets
* ðŸ““ [scb-mt-en-th-2020: A Large English-Thai Parallel Corpus](./benchmarks/2020-Lowphansirikul_scb-mt-en-th-2020.md)
* ðŸ““ [TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages](./benchmarks/2020-Clark_TyDi-QA.md)
* ðŸ““ [XNLI: Evaluating Cross-lingual Sentence Representations](./benchmarks/2018-Conneau_XNLI.md)



## Thai / other Asian languages
* ðŸ““ [Note to myself](./thai-and-neighbours/note.md)
* ðŸŽ“ Kawtrakul et al., 2002 [A State of the Art of Thai Language Resources and Thai Language Behavior Analysis and Modeling](./thai-and-neighbours/2002-Kawtrakul__SOTA-of-thai-language.md)

* ðŸ““ Bintang I., 2019 [Comparison FlairMulti, BERT, XLM, MuseCrossLingual and BytePairMultilingual Embeddings in case of Multilingual Task](https://medium.com/@nullphantom/comparison-flairmulti-bert-xlm-musecrosslingual-and-bytepairmultilingual-embeddings-in-case-of-a2dfb165f7b0) -- his article suggested that mBERT doesnâ€™t perform well in Thai and Chinese.

* ðŸ““ Tanruangporn P., 2017 [Experimenting with Neural Machine Translation for Thai](./thai-and-neighbours/2017-Tanruangporn_exo-with-NMT.md)


## Transfer learning in general

#### BERTology
* ðŸ““ [A Primer in BERTology: What we know about how BERT works](./otherwise/2020-Rogers_BERTology.md) \#\# `BERT`

## Otherwise


# Future Reading

## BERTology
1.  Gregor Wiedemann, Steffen Remus, Avi Chawla, and Chris Biemann. 2019. Does BERT make any sense? interpretable word sense disambiguation with contextualized embeddings.
2.  Allyson Ettinger. 2019. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models
3.  Clark et al. 2019. What does BERT look at? An analysis of BERTâ€™s attention
    

### Multi-lingual BERT-related
1.  Libovicky et al. 2019. How Language-Neutral is Multilingual BERT?
2.  Artetxe et al. 2019. On the Cross-lingual Transferability of Monolingual Representations.
3.  Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of bert.
4.  Ke Tran. 2019. From English to Foreign Languages: Transferring Pre-trained Language Models.

### Cross-lingual model
1.  FastText: Multi-lingual wordvector
2.  Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Wei-Ying Ma. 2016a. Dual learning for machine translation
3.  Jiajun Zhang and Chengqing Zong. 2016. Exploiting source-side monolingual data in neural machine translation
4.  Alina Karakanta, Jon Dehdari, and Josef van Genabith. 2017. Neural machine translation for low-resource languages without parallel corpora
5.  Anna Currey, Antonio Valerio Miceli Barone, and Kenneth Heafield. 2017. Copied Monolingual Data Improves Low-Resource Neural Machine Translation
6.  Xing Niu, Michael Denkowski, and Marine Carpuat. 2018. Bi-directional neural machine translation with synthetic parallel data
7.  Word Translation Without Parallel Data
8. Kim Y. and Rush A. (2016) Sequence-Level Knowledge Distillation
9. LibovickÃ½ et al. (2019) How Language-Neutral is Multilingual BERT?
10. Artetxe et al. (2019) On the Cross-lingual Transferability of Monolingual Representations
11. Tran K. (2020) From English to Foreign Languages: Transferring Pre-trained Language Models
12. Wu S. and Dredze M. (2019) Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT
13. Wang et al. (2020) Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework
14. SuÃ¡rez et al. (2020) A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages
15. Fan et al. (2020) Beyond English-Centric Multilingual Machine Translation
16. Ruder et al. (2019) A Survey Of Cross-lingual Word Embedding Models

    
  
### Related to Thai and Asian languages
1. Qin et al. (2020) CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot Cross-Lingual NLP
2. Horsuwan et al (2019) A Comparative Study of Pretrained Language Models on Thai Social Text Categorization
3. Aroonmanakun W. (2004) A Unified Model of Thai Romanization and Word Segmentation
4. Lertpiya et al. (2018) A Preliminary Study on Fundamental Thai NLP Tasks for User-generated Web Content
5. Saetia et al. (2019) Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations
6. Limkonchotiwat et al. (2020) Domain Adaptation of Thai Word Segmentation Models using Stacked Ensemble
7. Ding et al. (2018) Simplified Abugidas
8. Ding et al. (2016) Similar Southeast Asian Languages: Corpus-Based Case Study on Thai-Laotian and Malay-Indonesian
  

### Typological linguistic
1. Ponti et al. (2020) Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing
2. Pastor et al. (2018) Language Equality in the digital age.
3. Bender E. (2016) Linguistic typology in natural language processing    